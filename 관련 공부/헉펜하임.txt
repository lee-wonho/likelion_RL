1-1 reinforcement learning 도입
  example
    cart-pole
    alpha-go
    alpha-star
    walking-model

  action에 순차적인 결정

  *목적
    - maximize 'Reward'
1-2 Q-learning(기초)
  예시
    - 맛집 찾기
    Q(s,a) = max Q(s',a');a'
  greddy action 이용

  *exploration(탐험) epsilon-greedy
    1. 새로운 path
    2. 새로운 보상
    3. 학습이 불가 -> decaying epsilon-greedy (epsilon을 줄여나감)
  exploitation(활용)
  trade-off

  *gamma(discount factor)를 쓰는 이유
    - 효율적인 path를 찾게 해줌(다음 sequential한 action의 개수가 적고 reward가 큰 쪽으로)

  Q-update
    Q(s,a) <- (1-alpha)Q(s,a) + alpha(r +gamma max Q(s',a')) (0<= alpha <=1)
    alpha - 새로운 걸 받아들이는 기준

2-1 MDP
  Markov Decision Process
    Decision = action
    s_0 -> a_0 -> s_1 -> a_1 -> .... -> s_T(terminated)
  PMF(Probability Mass Function)
  PDF(Probability Density Function)
  CMF(Cumulative Mass Function)
  CDF(Cumulative Density Function)

  P(a_1|s_0,a_0,s_1) = p(a_1|s_1) 정책 (policy)
  p(s_2|s_0,a_0,s_1,a_1) = p(s_2|s_1,a_1) 전이확률 (transfer)
  return G_t = R_t + gamma * R_t+1 + gamma^2 *R_t+2 + ....

2-2 V & Q & Optimal Policy
  state value function(V) 지금부터 기대되는 return
  action value function(Q) 지금 행동을 취했을 때 기대되는 return
  Optimal Policy 현재 V가 최대가 되는 정책

  V(s_t) = E[G_t|s_t]
         = Integral [G_t * P(a_t,s_t+1,a_t+1,s_t+2,...|s_t)] da_t:a_T
  Q(s_t,a_t) = E[G_t|s_t,a_t]
             = Integral [G_t * P(s_t+1,a_t+1,s_t+2,...|s_t,a_t)] ds_t+1:a_T
2-3 Bellman equation
  p(x,y) = p(x|y)p(y)
  p(x,y|z) = p(x|y,z)p(y|z)
  V(s_t) = E[G_t|s_t]
         = Integral [G_t * P(a_t,s_t+1,a_t+1,s_t+2,...|s_t)] da_t:a_T
         = Integral [Integral[G_t * p(s_t+1,a_t+1,s_t+2,...|s_t,a_t)]ds_t+1:a_T p(a_t|s_t)]da_t
         = Integral [Q(s_t,a_t)p(a_t|s_t)]da_t
         = Integral [Integral[G_t * p(a_t+1,s_t+2,...|s_t,a_t,s_t+1)]da_t+1:a_T p(a_t|s_t)p(a_t,s_t+1|s_t)]da_t,s_t+1
         = Integral [Integral[(R_t+gamma*G_t+1) * p(a_t+1,s_t+2,...|s_t+1)]da_t+1:a_T p(a_t|s_t)p(a_t,s_t+1|s_t)]da_t,s_t+1
         = Integral [R_t+ gamma*V(s_t+1)p(a_t|s_t)p(a_t,s_t+1|s_t)]da_t,s_t+1

  Q(s_t,a_t) = E[G_t|s_t,a_t]
             = Integral [G_t * P(s_t+1,a_t+1,s_t+2,...|s_t,a_t)] ds_t+1:a_T
             = Integral [Integral [(R_t+gamma*G_t+1)p(a_t+1,s_t+2,...|s_t+1)]da_t+1:a_T p(s_t+1|s_t,a_t)]ds_t+1
             = Integral [(R_t+gamma*V(s_t+1))p(s_t+1|s_t,a_t)]ds_t+1
             = Integral [Integral [(R_t+gamma*G_t+1)p(s_t+2,...|s_t+1,a_t+1)]ds_t+2:a_T p(s_t+1,a_t+1|s_t,a_t)]ds_t+1,a_t+1
             = Integral [Integral [(R_t+gamma*G_t+1)p(s_t+2,...|s_t+1,a_t+1)]ds_t+2:a_T p(s_t+1)p(s_t+1|s_t,a_t)]ds_t+1,a_t+1
             = Integral [(R_t+gamma*Q(s_t+1,a_t+1))p(s_t+1,a_t+1|s_t,a_t)]ds_t+1,a_t+1

3-1 Optimal Policy
  optimal policy = policy to maximize state value function
  V(s_t) = E[Q(s_t,a_t)p(a_t|s_t)|s_t]
  a*_t = argmax Q*(s_t,a_t);a_t
  p*(a_t|s_t) = delta(a_t - a*_t)

3-2 MC 방법
  Q*를 어떻게 얻을까
    점근적으로 근사
  E[x] = Integral [x p(x)]dx = sum[x_i];i /N
  Q(s_t,a_t) = Integral [G_t * P(s_t+1,a_t+1,s_t+2,...|s_t,a_t)] ds_t+1:a_T
             = sum(G^(i)_t);i /N
3-3 TD 방법
  Temporal Difference
    Q(s_t,a_t) = Integral[(R_t+gamma*Q(s_t+1,a_t+1))p(s_t+1|s_t,a_t)p(a_t+1|s_t+1)]ds_t+1,a_t+1
               = sum[(R^(i)_t + gamma*Q(s^(i)_t+1,a^(i)_t+1))];i / N
               = ((N-1)*Q_N-1 + R^(N)_t + gamma(s^(N)_t+1,a^(N)_t+1)) / N
               = Q_N-1 + (R^(N) + gamma*Q(s^(N)_t+1,a^(N)_t+1) - Q_N-1) / N : Incremental Monte Carlo update
               = (1-alpha)Q_N-1 + alpha*(R^(N)_t + gamma*Q(s^(N)_t+1,a^(N)_t+1))
    *TD error : R^(N) + gamma*Q(s^(N)_t+1,a^(N)_t+1) - Q_N-1
    *TD target : R^(N) + gamma*Q(s^(N)_t+1,a^(N)_t+1)
  SARSA (s,a,r,s',a')

3-4 MC vs TD
  *MC
    1. non-biased
    2. high variance
    G를 sampling (완전한 G)
  *TD
    1. biased
    2. low variance
    Q를 sampling (불완전한 Q) -> bias 발생

4-1 On-Policy vs Off-Policy
  On-Policy
    Behavior policy = Target policy
  Off-Policy
    Behavior policy != Target Policy

  in brief
    Behavior policy = a_t
    Target Policy = a_t+1

  *Off-Policy의 장점
    1. 사람 or other agents
    2. exploration을 마음껏 하면서 optimal(greedy)로 sampling(TD_target)
    3. 재평가 가능 (오목 예시)

4-2 Q-learning
  behavior policy(q) - epsilon-greedy
  target policy - greedy p(a_t+1|s_t+1) = delta(a_t+1-a^*_t+1)

  target
    Integral[(R_t+gamma*Q(s_t+1,a_t+1))p(s_t+1|s_t,a_t)delta(a_t+1-a^*_t+1)]ds_t+1,a_t+1
    = Integral [(R_t + gamma*max[Q(s_t+1,a_t+1)])p(s_t+1|s_t,a_t)]ds_t+1
    = R^(N)_t + gamma*max[Q(s^(N)_t+1,a_t+1)]

4-3 SARSA vs Q-learning
  SARSA
    - 음수 reward도 전파됨
    - 안정적인 path 추정
  Q-learning
    - max-reward로 전파
    - 음수의 reward는 전파되지 않음

4-4 n-step TD
  Q(s_t,a_t) = Integral [(R_t+gamma*R_t+1 + gamma^2 *Q(s_t+2,a_t+2))
                        p(a_t+2|s_t+2)p(s_t+2|s_t+1,a_t+1)p(a_t+1|s_t+1)p(s_t+1|s_t,a_t)]ds_t+1,a_t+1,s_t+2,a_t+2
  p(s_t+2|s_t+1,a_t+1) , p(s_t+1|s_t,a_t) -> next state를 얻기 위해 q에서 샘플링

  importance sampling (p/q) discrete한 환경에서만 사용
